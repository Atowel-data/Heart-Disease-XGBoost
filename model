import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn import svm, datasets
from sklearn import metrics
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.metrics import roc_auc_score
#Data provided by kaggle and can be found here
#https://www.kaggle.com/fedesoriano/heart-failure-prediction
#download link:https://www.kaggle.com/fedesoriano/heart-failure-prediction/download
df = pd.read_csv(r'C:\Users\User\Downloads\heart.csv')

def missing (df):
    missing_number = df.isnull().sum().sort_values(ascending=False)
    missing_percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)
    missing_values = pd.concat([missing_number, missing_percent], axis=1, keys=['Missing_Number', 'Missing_Percent'])
    return missing_values

missing(df)
numerical= df.drop(['HeartDisease'], axis=1).select_dtypes('number').columns

categorical = df.select_dtypes('object').columns

print(f'Numerical Columns:  {df[numerical].columns}')
print('\n')
print(f'Categorical Columns: {df[categorical].columns}')

#prints the unique characters for categorical variables
df[categorical].nunique()

#prints number of people with(1) and without(0) heart disease
total_HD = df.HeartDisease.value_counts()
print(total_HD)
pct_HD = df.HeartDisease.value_counts(normalize=True)
print(pct_HD)

#Create dummy variable for categorical variables
df_dummies = pd.get_dummies (data=df, columns=['Sex','ChestPainType', 'ExerciseAngina', 'ST_Slope', 'RestingECG' ])
#Partition data into predictor variables and target variable
X = df_dummies.drop("HeartDisease", axis=1)
Y = df_dummies.HeartDisease

#import for xgboost if needed
#import pip
#pip.main(['install', 'xgboost'])

#Partition data into train and test sets
seed = 7
test_size = 0.33
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)

# fit model 
model = XGBClassifier()
model.fit(X_train, y_train)

# make predictions for test data
y_pred = model.predict(X_test)
predictions = [round(value) for value in y_pred]

# evaluate predictions
accuracy = accuracy_score(y_test, predictions)
print("Accuracy: %.2f%%" % (accuracy * 100.0))

#Hyperparameter tuning
param_grid = {
    "max_depth": [3, 4, 5, 7],
    "learning_rate": [0.1, 0.01, 0.05],
    "gamma": [0, 0.25, 1],
    "reg_lambda": [0, 1, 10],
    "scale_pos_weight": [1, 3, 5],
    "subsample": [0.8],
    "colsample_bytree": [0.5],
}
from sklearn.model_selection import GridSearchCV

# Init classifier
xgb_cl = xgb.XGBClassifier(objective="binary:logistic")

# Init Grid Search
grid_cv = GridSearchCV(xgb_cl, param_grid, n_jobs=-1, cv=3, scoring="roc_auc")

# Fit
_ = grid_cv.fit(X, Y)

#prints best score possible from hyperparameter tuning
grid_cv.best_score_

#prints best hyperparamter tuning grid
grid_cv.best_params_

#Hyperparameter tuning
param_grid["scale_pos_weight"] = [3]
param_grid["subsample"] = [0.8]
param_grid["colsample_bytree"] = [0.5]

# Give new value ranges to other params
param_grid["gamma"] = [3, 5, 7]
param_grid["max_depth"] = [5, 10, 15]
param_grid["reg_lambda"] = [0]
param_grid["learning_rate"] = [0.1, 0.2, 0.3, 0.4]

grid_cv_2 = GridSearchCV(model, param_grid, 
                         cv=3, scoring="roc_auc", n_jobs=-1)

_ = grid_cv_2.fit(X, Y)

#Scoring second model
grid_cv_2.best_score_

#Predict X_test set for heart disease
_ = grid_cv_2.fit(X_train, y_train)

preds = grid_cv_2.predict(X_test)
print(preds)

#ROC curve
clf = LogisticRegression(penalty='l2', C=0.1)
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
y_pred_proba = clf.predict_proba(X_test)[::,1]
fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)
auc = metrics.roc_auc_score(y_test, y_pred_proba)
plt.plot(fpr,tpr,label="data 1, auc="+str(auc))
plt.legend(loc=4)
plt.show()
